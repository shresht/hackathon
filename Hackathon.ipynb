{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}\n",
    "\n",
    "MAX_LEN = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_labels(target, label_index=label_index, one_hot=True):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = preprocess_labels(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = {v:[x_train[x] for x in range(8000) if y_train[x] == v] for v in label_index.values()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data to represent all classes equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution = np.unique(y_train,return_counts=1,)[-1]/len(y_train)\n",
    "inverse = (1/distribution)\n",
    "inverse = inverse/inverse.sum()\n",
    "\n",
    "\n",
    "for i in range(8000):\n",
    "    random_class = np.random.choice(10, p=inverse)\n",
    "    \n",
    "    sample_class = train_data[random_class]\n",
    "    sample = sample_class[np.random.choice(len(sample_class))]\n",
    "    \n",
    "    x_train.append(sample)\n",
    "    y_train.append(random_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0978125,  0.1013125,  0.1011875,  0.097    ,  0.096    ,\n",
       "        0.105625 ,  0.092125 ,  0.09925  ,  0.114375 ,  0.0953125])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=1)[-1]/16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We need to convert the data such that it makes more sense for a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Extract the e1 and e2 words\n",
    "\n",
    "![](http://i.imgur.com/pmSubOi.png)\n",
    "\n",
    "**Input :** `They saw that the <e1>equipment</e1> was put inside rollout <e2>drawers</e2>`.\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`They saw that the equipment was put inside rollout drawers.\n",
    "[equipment, drawers]`\n",
    "\n",
    "The sentence with the markup `<e1>.....</e1>` and `<e2>.....</e2>` are first converted to normal text. The e1 and e2 words are stored for each data sample in a separate array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove all stopwords and punctuation\n",
    "**Input :**  \n",
    "`They saw that the equipment was put inside rollout drawers. \n",
    "[equipment, drawers]`\n",
    "\n",
    "**Output :**  \n",
    "`They saw that the equipment was put inside rollout drawers \n",
    "[equipment, drawers]`\n",
    "\n",
    "## Step 3: Tokenize the words\n",
    "\n",
    "**Input :**  They saw that the equipment was put inside rollout drawers. [equipment, drawers]\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "All the words are given an index and are converted to numbers for easier processing\n",
    "\n",
    "## Step 4: Make a one-hot-encoding of e1 and e2 words (Almost one-hot. There are some e1 and e2 phrases\n",
    "\n",
    "![](http://i.imgur.com/TjTnAnT.png)\n",
    "\n",
    "**Input :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pad all sequences to equal lengths (85 words)\n",
    "\n",
    "**Input : **\n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]`\n",
    "\n",
    "**Output :**\n",
    "\n",
    "`[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert labels into one-hot-encoded vectors\n",
    "\n",
    "**Input: **\n",
    "`Cause-Effect`\n",
    "\n",
    "**Output: **\n",
    "`[1,0,0,0,0,0,0,0,0,0]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(lab):\n",
    "    labels = keras.utils.to_categorical(lab)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all this now. This might take some time depending on the performance of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (16000, 85)\n",
      "Labels: (16000, 10)\n",
      "\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (2717, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "y_train = onehot(y_train)\n",
    "\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "y_test = onehot(preprocess_labels(y_test))\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Word embeddings convert words into vectors that give it semantic meaning. These vectors are dense representation of the words and make more sense to a machine learning program. [Here](https://www.tensorflow.org/tutorials/word2vec) is a great article to get started. \n",
    "\n",
    "**Word Embeddings are essential, for our model to perform well**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the GLOVE word embeddings Matrix as a txt file. Download it [here](https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M). Place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found. Loading word2vec matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "link = \"https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M\"\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    print(\"File not found in the directory.\\nPlease download %s\" % link)\n",
    "    print(\"Make sure to place it in the same directory as this notebook\")\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the e1 and e1 positional vectors with the main sentence\n",
    "\n",
    "In all of the models below, the e1 and e2 positional vectors are concatenated with the main sentence vector.\n",
    "\n",
    "![](http://i.imgur.com/culDn1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 1 : Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        word_vec = np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X[0]\n",
    "        ])\n",
    "        \n",
    "        merged = np.concatenate([word_vec, X[1], X[2]], axis=1)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=500))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest = Pipeline([\n",
    "    (\"word2vec\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"random forest\", RandomForestClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word2vec', <__main__.MeanEmbeddingVectorizer object at 0x000002386BC20BA8>), ('random forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurit...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.fit(x_train,y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2717, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21751932278248068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_forest.predict(x_test) == np.argmax(y_test,axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_classifier =  Pipeline([\n",
    "    (\"word2vec\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"random forest\", c)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word2vec', <__main__.MeanEmbeddingVectorizer object at 0x000002380201DA58>), ('random forest', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_classifier.fit(x_train,y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2717, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21899153478100847"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(svc_classifier.predict(x_test) == np.argmax(y_test,axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost as bad. Just little better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 3 : Deep LSTM RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Input\n",
    "The first layer of our network will take one input : **The main sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Word Embeddings\n",
    "The sentence input will be converted to a vector based on the word2vec matrix that we obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_layer = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed_layer(sentence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auxillary Inputs\n",
    "\n",
    "We will feed the e1 and e2 positional vectors to the word embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Merge Layer\n",
    "This concatanates the values from the the two e1 and e2 vectors as auxillary input with the word embedding vectors.\n",
    "\n",
    "`merged layer = e1 +e2 + word embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Dropout\n",
    "To prevent overfitting, half of the incoming inputs will be sqashed to 0. Yeah. It's savage and it's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: LSTM Neurons 1\n",
    "We added 128 LSTM neurons that returns requences into the next LSTM array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Dropout\n",
    "Oh but before that. Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: LSTM Neurons 2\n",
    "Another 32 LSTM neurons that recieve from the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8: Dropout\n",
    "Of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 9: Softmax\n",
    "The final layer that outputs our prediction is a softmax layer with 10 neurons. One for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "Our model is ready. Let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            20608       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2,099,810 trainable parameters. That's a lot of room for over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train\n",
    "We can start the training process. All the datasamples from the train folder are being used. \n",
    "\n",
    "The test samples are **not** used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 2717 samples\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 159s - loss: 1.8404 - acc: 0.3713 - val_loss: 1.4513 - val_acc: 0.5245\n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 159s - loss: 1.4551 - acc: 0.5409 - val_loss: 1.2607 - val_acc: 0.5915\n",
      "Epoch 3/20\n",
      "16000/16000 [==============================] - 165s - loss: 1.2587 - acc: 0.6098 - val_loss: 1.2078 - val_acc: 0.6124\n",
      "Epoch 4/20\n",
      "16000/16000 [==============================] - 160s - loss: 1.1377 - acc: 0.6480 - val_loss: 1.1520 - val_acc: 0.6342\n",
      "Epoch 5/20\n",
      "16000/16000 [==============================] - 162s - loss: 1.0299 - acc: 0.6821 - val_loss: 1.1113 - val_acc: 0.6415\n",
      "Epoch 6/20\n",
      "16000/16000 [==============================] - 163s - loss: 0.9449 - acc: 0.7131 - val_loss: 1.1436 - val_acc: 0.6551\n",
      "Epoch 7/20\n",
      "16000/16000 [==============================] - 155s - loss: 0.8578 - acc: 0.7374 - val_loss: 1.1525 - val_acc: 0.6522\n",
      "Epoch 8/20\n",
      "16000/16000 [==============================] - 160s - loss: 0.7847 - acc: 0.7576 - val_loss: 1.1769 - val_acc: 0.6680\n",
      "Epoch 9/20\n",
      "16000/16000 [==============================] - 162s - loss: 0.7414 - acc: 0.7746 - val_loss: 1.1427 - val_acc: 0.6665\n",
      "Epoch 10/20\n",
      "16000/16000 [==============================] - 164s - loss: 0.6861 - acc: 0.7914 - val_loss: 1.2138 - val_acc: 0.6640\n",
      "Epoch 11/20\n",
      "16000/16000 [==============================] - 161s - loss: 0.6367 - acc: 0.8066 - val_loss: 1.1494 - val_acc: 0.6695\n",
      "Epoch 12/20\n",
      "  832/16000 [>.............................] - ETA: 153s - loss: 0.5428 - acc: 0.8329"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6448ed89cede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove validation data\n",
    "model.fit(x_train,y_train, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the other models, this one performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the predicted results for the first 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_predict(array):\n",
    "    reverse_label = {v:k for k,v in label_index.items()}\n",
    "    return reverse_label[np.argmax(prediction)]\n",
    "\n",
    "a = 8000\n",
    "for prediction in model.predict(x_test)[:50]:\n",
    "    a+=1\n",
    "    print(a,format_predict(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "a = 8000\n",
    "for prediction in model.predict(x_test)[:50]:\n",
    "    a+=1\n",
    "    results.append(a)\n",
    "    results.append('\\t')\n",
    "    results.append(format_predict(prediction))\n",
    "    results.append('\\n')\n",
    "with open('results.txt','w') as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* More data\n",
    "* Maybe reduction of the model complexity\n",
    "* Using other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
