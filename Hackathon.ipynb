{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}\n",
    "\n",
    "MAX_LEN = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_labels(target, label_index=label_index, one_hot=True):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = preprocess_labels(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = {v:[x_train[x] for x in range(8000) if y_train[x] == v] for v in label_index.values()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data to represent all classes equally\n",
    "We are doubling the size of the dataset, from 8000 to 16000 by adding random duplicates of the classes that make a lesser representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution = np.unique(y_train,return_counts=1,)[-1]/len(y_train)\n",
    "inverse = (1/distribution)\n",
    "inverse = inverse/inverse.sum()\n",
    "\n",
    "\n",
    "for i in range(8000):\n",
    "    random_class = np.random.choice(10, p=inverse)\n",
    "    \n",
    "    sample_class = train_data[random_class]\n",
    "    sample = sample_class[np.random.choice(len(sample_class))]\n",
    "    \n",
    "    x_train.append(sample)\n",
    "    y_train.append(random_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1006875,  0.096375 ,  0.0985625,  0.1010625,  0.09275  ,\n",
       "        0.1059375,  0.1003125,  0.0974375,  0.1143125,  0.0925625])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=1)[-1]/16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We need to convert the data such that it makes more sense for a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Extract the e1 and e2 words\n",
    "\n",
    "![](http://i.imgur.com/pmSubOi.png)\n",
    "\n",
    "**Input :** `They saw that the <e1>equipment</e1> was put inside rollout <e2>drawers</e2>`.\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`They saw that the equipment was put inside rollout drawers.\n",
    "[equipment, drawers]`\n",
    "\n",
    "The sentence with the markup `<e1>.....</e1>` and `<e2>.....</e2>` are first converted to normal text. The e1 and e2 words are stored for each data sample in a separate array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove all stopwords and punctuation\n",
    "**Input :**  \n",
    "`They saw that the equipment was put inside rollout drawers. \n",
    "[equipment, drawers]`\n",
    "\n",
    "**Output :**  \n",
    "`They saw that the equipment was put inside rollout drawers \n",
    "[equipment, drawers]`\n",
    "\n",
    "## Step 3: Tokenize the words\n",
    "\n",
    "**Input :**  They saw that the equipment was put inside rollout drawers. [equipment, drawers]\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "All the words are given an index and are converted to numbers for easier processing\n",
    "\n",
    "## Step 4: Make a one-hot-encoding of e1 and e2 words (Almost one-hot. There are some e1 and e2 phrases\n",
    "\n",
    "![](http://i.imgur.com/TjTnAnT.png)\n",
    "\n",
    "**Input :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pad all sequences to equal lengths (85 words)\n",
    "\n",
    "**Input : **\n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]`\n",
    "\n",
    "**Output :**\n",
    "\n",
    "`[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert labels into one-hot-encoded vectors\n",
    "\n",
    "**Input: **\n",
    "`Cause-Effect`\n",
    "\n",
    "**Output: **\n",
    "`[1,0,0,0,0,0,0,0,0,0]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(lab):\n",
    "    labels = keras.utils.to_categorical(lab)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all this now. This might take some time depending on the performance of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (16000, 85)\n",
      "Labels: (16000, 10)\n",
      "\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (2717, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "y_train = onehot(y_train)\n",
    "\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "y_test = onehot(preprocess_labels(y_test))\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Word embeddings convert words into vectors that give it semantic meaning. These vectors are dense representation of the words and make more sense to a machine learning program. [Here](https://www.tensorflow.org/tutorials/word2vec) is a great article to get started. \n",
    "\n",
    "**Word Embeddings are essential, for our model to perform well**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the GLOVE word embeddings Matrix as a txt file. Download it [here](https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M). Place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found. Loading word2vec matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "link = \"https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M\"\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    print(\"File not found in the directory.\\nPlease download %s\" % link)\n",
    "    print(\"Make sure to place it in the same directory as this notebook\")\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the e1 and e1 positional vectors with the main sentence\n",
    "\n",
    "In all of the models below, the e1 and e2 positional vectors are concatenated with the main sentence vector.\n",
    "\n",
    "![](http://i.imgur.com/culDn1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 1 : Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        word_vec = np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X[0]\n",
    "        ])\n",
    "        \n",
    "        merged = np.concatenate([word_vec, X[1], X[2]], axis=1)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=500))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest = Pipeline([\n",
    "    (\"word2vec\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"random forest\", RandomForestClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word2vec', <__main__.MeanEmbeddingVectorizer object at 0x000002242FB958D0>), ('random forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurit...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.fit(x_train,y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26021347073978651"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_forest.predict(x_test) == np.argmax(y_test,axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_classifier =  Pipeline([\n",
    "    (\"word2vec\", MeanEmbeddingVectorizer(embeddings_index)),\n",
    "    (\"random forest\", c)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word2vec', <__main__.MeanEmbeddingVectorizer object at 0x000002242FEA6E10>), ('random forest', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_classifier.fit(x_train,y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21494295178505704"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(svc_classifier.predict(x_test) == np.argmax(y_test,axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost as bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model attempt 3 : Deep LSTM RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Input\n",
    "The first layer of our network will take one input : **The main sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Word Embeddings\n",
    "The sentence input will be converted to a vector based on the word2vec matrix that we obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_layer = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed_layer(sentence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auxillary Inputs\n",
    "\n",
    "We will feed the e1 and e2 positional vectors to the word embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Merge Layer\n",
    "This concatanates the values from the the two e1 and e2 vectors as auxillary input with the word embedding vectors.\n",
    "\n",
    "`merged layer = e1 +e2 + word embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Dropout\n",
    "To prevent overfitting, half of the incoming inputs will be sqashed to 0. Yeah. It's savage and it's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: LSTM Neurons 1\n",
    "We added 128 LSTM neurons that returns requences into the next LSTM array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Dropout\n",
    "Oh but before that. Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: LSTM Neurons 2\n",
    "Another 32 LSTM neurons that recieve from the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8: Dropout\n",
    "Of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 9: Softmax\n",
    "The final layer that outputs our prediction is a softmax layer with 10 neurons. One for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "Our model is ready. Let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            20608       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2,099,810 trainable parameters. That's a lot of room for over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train\n",
    "We can start the training process. All the datasamples from the train folder are being used. \n",
    "\n",
    "This will take a long time. About 30 mins on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "16000/16000 [==============================] - 160s - loss: 1.8488 - acc: 0.3730   \n",
      "Epoch 2/12\n",
      "16000/16000 [==============================] - 161s - loss: 1.4462 - acc: 0.5405   \n",
      "Epoch 3/12\n",
      "16000/16000 [==============================] - 159s - loss: 1.2636 - acc: 0.6031   \n",
      "Epoch 4/12\n",
      "16000/16000 [==============================] - 163s - loss: 1.1340 - acc: 0.6504   \n",
      "Epoch 5/12\n",
      "16000/16000 [==============================] - 163s - loss: 1.0323 - acc: 0.6851   \n",
      "Epoch 6/12\n",
      "16000/16000 [==============================] - 166s - loss: 0.9406 - acc: 0.7137   \n",
      "Epoch 7/12\n",
      "16000/16000 [==============================] - 165s - loss: 0.8696 - acc: 0.7321   \n",
      "Epoch 8/12\n",
      "16000/16000 [==============================] - 165s - loss: 0.7972 - acc: 0.7554   \n",
      "Epoch 9/12\n",
      "16000/16000 [==============================] - 165s - loss: 0.7419 - acc: 0.7730   \n",
      "Epoch 10/12\n",
      "16000/16000 [==============================] - 165s - loss: 0.6854 - acc: 0.7904   \n",
      "Epoch 11/12\n",
      "16000/16000 [==============================] - 165s - loss: 0.6326 - acc: 0.8083   \n",
      "Epoch 12/12\n",
      "16000/16000 [==============================] - 157s - loss: 0.5900 - acc: 0.8179   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22435531940>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove validation data\n",
    "model.fit(x_train,y_train, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717/2717 [==============================] - 6s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67206477721824687"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the other models, this one performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the predicted results for the first 5 test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8001 Message-Topic\n",
      "8002 Product-Producer\n",
      "8003 Instrument-Agency\n",
      "8004 Entity-Destination\n",
      "8005 Cause-Effect\n"
     ]
    }
   ],
   "source": [
    "def format_predict(array):\n",
    "    reverse_label = {v:k for k,v in label_index.items()}\n",
    "    return reverse_label[np.argmax(prediction)]\n",
    "\n",
    "a = 8000\n",
    "for prediction in model.predict(x_test)[:5]:\n",
    "    a+=1\n",
    "    print(a,format_predict(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty good at predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write all this to `result.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "a = 8000\n",
    "for prediction in model.predict(x_test):\n",
    "    a+=1\n",
    "    results.append(str(a))\n",
    "    results.append('\\t')\n",
    "    results.append(format_predict(prediction))\n",
    "    results.append('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ''.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt','w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* More data\n",
    "* Maybe reduction of the model complexity\n",
    "* Using other dimension word embeddings, especially the 50d one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
