{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}\n",
    "\n",
    "MAX_LEN = 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We need to convert the data such that it makes more sense for a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Extract the e1 and e2 words\n",
    "\n",
    "**Input :** `They saw that the <e1>equipment</e1> was put inside rollout <e2>drawers</e2>`.\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`They saw that the equipment was put inside rollout drawers.\n",
    "[equipment, drawers]`\n",
    "\n",
    "The sentence with the markup `<e1>.....</e1>` and `<e2>.....</e2>` are first converted to normal text. The e1 and e2 words are stored for each data sample in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove all stopwords and punctuation\n",
    "**Input :**  \n",
    "`They saw that the equipment was put inside rollout drawers. \n",
    "[equipment, drawers]`\n",
    "\n",
    "**Output :**  \n",
    "`They saw that the equipment was put inside rollout drawers \n",
    "[equipment, drawers]`\n",
    "\n",
    "## Step 3: Tokenize the words\n",
    "\n",
    "**Input :**  They saw that the equipment was put inside rollout drawers. [equipment, drawers]\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "All the words are given an index and are converted to numbers for easier processing\n",
    "\n",
    "## Step 4: Make a one-hot-encoding of e1 and e2 words (Almost one-hot. There are some e1 and e2 phrases\n",
    "\n",
    "**Input :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pad all sequences to equal lengths (85 words)\n",
    "\n",
    "**Input : **\n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]`\n",
    "\n",
    "**Output :**\n",
    "\n",
    "`[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert labels into one-hot-encoded vectors\n",
    "\n",
    "**Input: **\n",
    "`Cause-Effect`\n",
    "\n",
    "**Output: **\n",
    "`[1,0,0,0,0,0,0,0,0,0]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_labels(target, label_index=label_index):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    labels = keras.utils.to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all this now. This might take some time depending on the performance of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (8000, 85)\n",
      "Labels: (8000, 10)\n",
      "\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (8000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "\n",
    "y_train = preprocess_labels(y_train)\n",
    "y_test = preprocess_labels(y_test)\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([ 717, 1410,  634,  690,  504,  716,  845,  540,  941, 1003], dtype=int64))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word embeddings convert words into vectors that give it semantic meaning. These vectors are dense representation of the words and make more sense to a machine learning program. [Here](https://www.tensorflow.org/tutorials/word2vec) is a great article to get started. \n",
    "\n",
    "**Word Embeddings are essential, for our model to perform well**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the GLOVE word embeddings Matrix as a txt file. Download it [here](https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M). Place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found. Loading word2vec matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "link = \"https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M\"\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    print(\"File not found in the directory.\\nPlease download %s\" % link)\n",
    "    print(\"Make sure to place it in the same directory as this notebook\")\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model : Deep LSTM RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Input\n",
    "The first layer of our network will take one input : **The main sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Word Embeddings\n",
    "The sentence input will be converted to a vector based on the word2vec matrix that we obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_layer = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed_layer(sentence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auxillary Inputs\n",
    "\n",
    "We will feed the e1 and e2 positional vectors to the word embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Merge Layer\n",
    "This concatanates the values from the the two e1 and e2 vectors as auxillary input with the word embedding vectors.\n",
    "\n",
    "`merged layer = e1 +e2 + word embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Dropout\n",
    "To prevent overfitting, half of the incoming inputs will be sqashed to 0. Yeah. It's savage and it's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: LSTM Neurons 1\n",
    "We added 128 LSTM neurons that returns requences into the next LSTM array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Dropout\n",
    "Oh but before that. Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: LSTM Neurons 2\n",
    "Another 32 LSTM neurons that recieve from the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8: Dropout\n",
    "Of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 9: Softmax\n",
    "The final layer that outputs our prediction is a softmax layer with 10 neurons. One for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "Our model is ready. Let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            20608       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2,099,810 trainable parameters. That's a lot of room for over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train\n",
    "We can start the training process. All the datasamples from the train folder are being used. \n",
    "\n",
    "The test samples are **not** used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 72s - loss: 1.9979 - acc: 0.2955 - val_loss: 1.6843 - val_acc: 0.4361\n",
      "Epoch 2/20\n",
      "1024/8000 [==>...........................] - ETA: 58s - loss: 1.7887 - acc: 0.3604"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6448ed89cede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove validation data\n",
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_predict(array):\n",
    "    reverse_label = {v:k for k,v in label_index.items()}\n",
    "    return reverse_label[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8001 Other\n",
      "8002 Entity-Destination\n",
      "8003 Other\n",
      "8004 Entity-Destination\n",
      "8005 Cause-Effect\n",
      "8006 Component-Whole\n",
      "8007 Other\n",
      "8008 Other\n",
      "8009 Content-Container\n",
      "8010 Message-Topic\n",
      "8011 Entity-Destination\n",
      "8012 Product-Producer\n",
      "8013 Entity-Destination\n",
      "8014 Other\n",
      "8015 Content-Container\n",
      "8016 Other\n",
      "8017 Other\n",
      "8018 Other\n",
      "8019 Member-Collection\n",
      "8020 Other\n",
      "8021 Message-Topic\n",
      "8022 Content-Container\n",
      "8023 Other\n",
      "8024 Member-Collection\n",
      "8025 Cause-Effect\n",
      "8026 Entity-Destination\n",
      "8027 Other\n",
      "8028 Other\n",
      "8029 Message-Topic\n",
      "8030 Other\n",
      "8031 Cause-Effect\n",
      "8032 Message-Topic\n",
      "8033 Component-Whole\n",
      "8034 Cause-Effect\n",
      "8035 Component-Whole\n",
      "8036 Component-Whole\n",
      "8037 Other\n",
      "8038 Content-Container\n",
      "8039 Cause-Effect\n",
      "8040 Cause-Effect\n",
      "8041 Message-Topic\n",
      "8042 Component-Whole\n",
      "8043 Message-Topic\n",
      "8044 Entity-Destination\n",
      "8045 Cause-Effect\n",
      "8046 Content-Container\n",
      "8047 Content-Container\n",
      "8048 Content-Container\n",
      "8049 Message-Topic\n",
      "8050 Member-Collection\n"
     ]
    }
   ],
   "source": [
    "a = 8000\n",
    "for prediction in model.predict(x_test)[:50]:\n",
    "    a+=1\n",
    "    print(a,format_predict(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* Data augmentation to make the classes equal\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
