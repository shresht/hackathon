{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}\n",
    "\n",
    "MAX_LEN = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_labels(target, label_index=label_index, one_hot=True):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = preprocess_labels(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {v:[x_train[x] for x in range(8000) if y_train[x] == v] for v in label_index.values()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment data to represent all classes equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution = np.unique(y_train,return_counts=1, axis=0)[-1]/y_train.shape[0]\n",
    "inverse = (1/distribution)\n",
    "inverse = inverse/inverse.sum()\n",
    "\n",
    "\n",
    "for i in range(8000):\n",
    "    random_class = np.random.choice(10, p=inverse)\n",
    "    \n",
    "    sample_class = train_data[random_class]\n",
    "    sample = sample_class[np.random.choice(len(sample_class))]\n",
    "    \n",
    "    x_train.append(sample)\n",
    "    y_train.append(random_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0980625,  0.0974375,  0.099125 ,  0.0955   ,  0.09725  ,\n",
       "        0.109375 ,  0.0971875,  0.0965625,  0.1143125,  0.0951875])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=1, axis=0)[-1]/16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We need to convert the data such that it makes more sense for a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Extract the e1 and e2 words\n",
    "\n",
    "**Input :** `They saw that the <e1>equipment</e1> was put inside rollout <e2>drawers</e2>`.\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`They saw that the equipment was put inside rollout drawers.\n",
    "[equipment, drawers]`\n",
    "\n",
    "The sentence with the markup `<e1>.....</e1>` and `<e2>.....</e2>` are first converted to normal text. The e1 and e2 words are stored for each data sample in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove all stopwords and punctuation\n",
    "**Input :**  \n",
    "`They saw that the equipment was put inside rollout drawers. \n",
    "[equipment, drawers]`\n",
    "\n",
    "**Output :**  \n",
    "`They saw that the equipment was put inside rollout drawers \n",
    "[equipment, drawers]`\n",
    "\n",
    "## Step 3: Tokenize the words\n",
    "\n",
    "**Input :**  They saw that the equipment was put inside rollout drawers. [equipment, drawers]\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "All the words are given an index and are converted to numbers for easier processing\n",
    "\n",
    "## Step 4: Make a one-hot-encoding of e1 and e2 words (Almost one-hot. There are some e1 and e2 phrases\n",
    "\n",
    "**Input :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[1022, 502]`\n",
    "\n",
    "**Output :** \n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pad all sequences to equal lengths (85 words)\n",
    "\n",
    "**Input : **\n",
    "\n",
    "`[23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502] \n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, , 0, 0, 0, 0, 1]`\n",
    "\n",
    "**Output :**\n",
    "\n",
    "`[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 54, 65, 1, 1022, 55, 66, 65, 1156, 502]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert labels into one-hot-encoded vectors\n",
    "\n",
    "**Input: **\n",
    "`Cause-Effect`\n",
    "\n",
    "**Output: **\n",
    "`[1,0,0,0,0,0,0,0,0,0]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(lab):\n",
    "    labels = keras.utils.to_categorical(lab)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all this now. This might take some time depending on the performance of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "y_train = onehot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = preprocess(clean_text(x_test))\n",
    "y_test = onehot(preprocess_labels(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (16000, 85)\n",
      "Labels: (16000, 10)\n",
      "\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (2717, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word embeddings convert words into vectors that give it semantic meaning. These vectors are dense representation of the words and make more sense to a machine learning program. [Here](https://www.tensorflow.org/tutorials/word2vec) is a great article to get started. \n",
    "\n",
    "**Word Embeddings are essential, for our model to perform well**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the GLOVE word embeddings Matrix as a txt file. Download it [here](https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M). Place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found. Loading word2vec matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "link = \"https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M\"\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    print(\"File not found in the directory.\\nPlease download %s\" % link)\n",
    "    print(\"Make sure to place it in the same directory as this notebook\")\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model : Deep LSTM RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Input\n",
    "The first layer of our network will take one input : **The main sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Word Embeddings\n",
    "The sentence input will be converted to a vector based on the word2vec matrix that we obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_layer = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed_layer(sentence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auxillary Inputs\n",
    "\n",
    "We will feed the e1 and e2 positional vectors to the word embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Merge Layer\n",
    "This concatanates the values from the the two e1 and e2 vectors as auxillary input with the word embedding vectors.\n",
    "\n",
    "`merged layer = e1 +e2 + word embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Dropout\n",
    "To prevent overfitting, half of the incoming inputs will be sqashed to 0. Yeah. It's savage and it's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: LSTM Neurons 1\n",
    "We added 128 LSTM neurons that returns requences into the next LSTM array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Dropout\n",
    "Oh but before that. Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: LSTM Neurons 2\n",
    "Another 32 LSTM neurons that recieve from the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8: Dropout\n",
    "Of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = keras.layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 9: Softmax\n",
    "The final layer that outputs our prediction is a softmax layer with 10 neurons. One for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "Our model is ready. Let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            20608       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2,099,810 trainable parameters. That's a lot of room for over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train\n",
    "We can start the training process. All the datasamples from the train folder are being used. \n",
    "\n",
    "The test samples are **not** used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 2717 samples\n",
      "Epoch 1/20\n",
      "16000/16000 [==============================] - 143s - loss: 1.8591 - acc: 0.3641 - val_loss: 1.4793 - val_acc: 0.5024\n",
      "Epoch 2/20\n",
      "16000/16000 [==============================] - 141s - loss: 1.4584 - acc: 0.5404 - val_loss: 1.3291 - val_acc: 0.5616\n",
      "Epoch 3/20\n",
      " 7488/16000 [=============>................] - ETA: 74s - loss: 1.2877 - acc: 0.6024"
     ]
    }
   ],
   "source": [
    "# Remove validation data\n",
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_predict(array):\n",
    "    reverse_label = {v:k for k,v in label_index.items()}\n",
    "    return reverse_label[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 8000\n",
    "for prediction in model.predict(x_test)[:50]:\n",
    "    a+=1\n",
    "    print(a,format_predict(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "* Data augmentation to make the classes equal\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
