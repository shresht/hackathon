{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the labels as well as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 85\n",
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)\n",
    "\n",
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]\n",
    "\n",
    "def preprocess_labels(target, label_index=label_index):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    labels = keras.utils.to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take some time depending on the performance of your system and internet speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "\n",
    "y_train = preprocess_labels(y_train)\n",
    "y_test = preprocess_labels(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a word embeddings Matrix (GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec matrix...\n"
     ]
    }
   ],
   "source": [
    "url = 'https://worksheets.codalab.org/rest/bundles/0x15a09c8f74f94a20bec0b68a2e6703b3/contents/blob/'\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        print \"Downloading %s\" % file_name\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in response.iter_content(chunk_size=4096):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50 * dl / total_length)\n",
    "                sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )    \n",
    "                sys.stdout.flush()\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (8000, 85)\n",
      "Labels: (8000, 10)\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (8000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Model with e1 and e2 as aux inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence input is being passed along with e1 and e2 as auxillary vectors. \n",
    "\n",
    "Eg: \n",
    "\n",
    "```arrayed <e1>configuration</e1> of antenna <e2>elements</e2>\n",
    "``` \n",
    "\n",
    "will be sent as \n",
    "\n",
    "|  |   |   |   |   |   \n",
    "|---|---|---|---|---|---\n",
    "||arrayed|configuration|of|antenna|elements|\n",
    "|*token*|10|44|5|24|104|\n",
    "|*e1*|0|1|0|0|0|\n",
    "|*e2*|0|0|0|0|1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 32)            20608       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 32)            0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")\n",
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)\n",
    "embed = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed(sentence_input)\n",
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")\n",
    "x = keras.layers.Dropout(0.2)(merged)\n",
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.LSTM(32)(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)\n",
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 83s - loss: 1.8075 - acc: 0.3606 - val_loss: 1.4674 - val_acc: 0.4965\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 82s - loss: 1.4224 - acc: 0.5149 - val_loss: 1.2750 - val_acc: 0.5594\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 82s - loss: 1.2522 - acc: 0.5680 - val_loss: 1.1906 - val_acc: 0.5918\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 78s - loss: 1.1371 - acc: 0.6087 - val_loss: 1.1456 - val_acc: 0.6058\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.0373 - acc: 0.6512 - val_loss: 1.1356 - val_acc: 0.6205\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 76s - loss: 0.9197 - acc: 0.6866 - val_loss: 1.1048 - val_acc: 0.6183\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 83s - loss: 0.8469 - acc: 0.7137 - val_loss: 1.1500 - val_acc: 0.6036\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 81s - loss: 0.7662 - acc: 0.7430 - val_loss: 1.1460 - val_acc: 0.6165\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 78s - loss: 0.6932 - acc: 0.7678 - val_loss: 1.1545 - val_acc: 0.6353\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 78s - loss: 0.6183 - acc: 0.7959 - val_loss: 1.1587 - val_acc: 0.6375\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 75s - loss: 0.5725 - acc: 0.8048 - val_loss: 1.1490 - val_acc: 0.6533\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 71s - loss: 0.5152 - acc: 0.8334 - val_loss: 1.2085 - val_acc: 0.6489\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 69s - loss: 0.4385 - acc: 0.8534 - val_loss: 1.2242 - val_acc: 0.6253\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 69s - loss: 0.4116 - acc: 0.8652 - val_loss: 1.3016 - val_acc: 0.6441\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 71s - loss: 0.3494 - acc: 0.8869 - val_loss: 1.3203 - val_acc: 0.6360\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 71s - loss: 0.3054 - acc: 0.9004 - val_loss: 1.3739 - val_acc: 0.6481\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 68s - loss: 0.2651 - acc: 0.9155 - val_loss: 1.4935 - val_acc: 0.6342\n",
      "Epoch 18/50\n",
      "3840/8000 [=============>................] - ETA: 33s - loss: 0.2296 - acc: 0.9315"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 79s - loss: 1.8166 - acc: 0.3669 - val_loss: 1.4517 - val_acc: 0.5101\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.4447 - acc: 0.5075 - val_loss: 1.3458 - val_acc: 0.5414\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 85s - loss: 1.2724 - acc: 0.5659 - val_loss: 1.2160 - val_acc: 0.5830\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.1499 - acc: 0.6138 - val_loss: 1.1568 - val_acc: 0.6117\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.0331 - acc: 0.6529 - val_loss: 1.0913 - val_acc: 0.6205\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 76s - loss: 0.9509 - acc: 0.6824 - val_loss: 1.1134 - val_acc: 0.6213\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 79s - loss: 0.8478 - acc: 0.7130 - val_loss: 1.1277 - val_acc: 0.6294\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 86s - loss: 0.7750 - acc: 0.7460 - val_loss: 1.0857 - val_acc: 0.6404\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 87s - loss: 0.6983 - acc: 0.7728 - val_loss: 1.1256 - val_acc: 0.6397\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 83s - loss: 0.6289 - acc: 0.7881 - val_loss: 1.1359 - val_acc: 0.6463\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 83s - loss: 0.5708 - acc: 0.8131 - val_loss: 1.1226 - val_acc: 0.6621\n",
      "Epoch 12/50\n",
      "2240/8000 [=======>......................] - ETA: 54s - loss: 0.4673 - acc: 0.8451"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
