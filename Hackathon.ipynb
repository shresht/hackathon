{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the labels as well as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 85\n",
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)\n",
    "\n",
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]\n",
    "\n",
    "def preprocess_labels(target, label_index=label_index):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    labels = keras.utils.to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take some time depending on the performance of your system and internet speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is exactly did we do?\n",
    "\n",
    "## Step 1 : Extract the e1 and e2 words\n",
    "**Input : They saw that the `<e1>equipment</e1>` was put inside rollout `<e2>drawers</e2>`**.\n",
    "\n",
    "\n",
    "The sentence with the markup `<e1>.....</e1>` and `<e2>.....</e2>` are first converted to normal text. The e1 and e2 words are stored for each data sample in a separate array.\n",
    "\n",
    "**Output : They saw that the equipment was put inside rollout drawers. [equipment, drawers]**\n",
    "\n",
    "## Step 2: Tokenize the words\n",
    "\n",
    "**Input :  They saw that the equipment was put inside rollout drawers. [equipment, drawers]**\n",
    "\n",
    "All the words are given an index and are converted to numbers for easier processing\n",
    "\n",
    "**Output : [23, 54, 65, 1, 45, 55, 66, 65, 56, 56, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "\n",
    "y_train = preprocess_labels(y_train)\n",
    "y_test = preprocess_labels(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a word embeddings Matrix (GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Found. Loading word2vec matrix...\n"
     ]
    }
   ],
   "source": [
    "link = \"https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-MmN2dVVkdnV1S2M\"\n",
    "file_name = \"glove.6B.100d.txt\"\n",
    "\n",
    "if not file_name in os.listdir('.'):\n",
    "    print(\"File not found in the directory.\\nPlease download %s\" % link)\n",
    "    print(\"Make sure to place it in the same directory as this notebook\")\n",
    "else:\n",
    "    print(\"File Found. Loading word2vec matrix...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename\n",
    "\n",
    "train_filename = maybe_download('glove.6B.100d.txt', 247336696)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt','rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (8000, 85)\n",
      "Labels: (8000, 10)\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (8000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Model with e1 and e2 as aux inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence input is being passed along with e1 and e2 as auxillary vectors. \n",
    "\n",
    "Eg: \n",
    "\n",
    "```arrayed <e1>configuration</e1> of antenna <e2>elements</e2>\n",
    "``` \n",
    "\n",
    "will be sent as \n",
    "\n",
    "|  |   |   |   |   |   \n",
    "|---|---|---|---|---|---\n",
    "||arrayed|configuration|of|antenna|elements|\n",
    "|*token*|10|44|5|24|104|\n",
    "|*e1*|0|1|0|0|0|\n",
    "|*e2*|0|0|0|0|1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 32)            20608       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")\n",
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)\n",
    "embed = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed(sentence_input)\n",
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")\n",
    "x = keras.layers.Dropout(0.5)(merged)\n",
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.LSTM(32)(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)\n",
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 42s - loss: 2.0000 - acc: 0.2916 - val_loss: 1.6260 - val_acc: 0.4354\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.7208 - acc: 0.4054 - val_loss: 1.4923 - val_acc: 0.4928\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.5808 - acc: 0.4649 - val_loss: 1.3363 - val_acc: 0.5436\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 40s - loss: 1.4737 - acc: 0.5060 - val_loss: 1.2452 - val_acc: 0.5738\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.4117 - acc: 0.5256 - val_loss: 1.2181 - val_acc: 0.5793\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.3293 - acc: 0.5565 - val_loss: 1.1470 - val_acc: 0.6246\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 40s - loss: 1.2725 - acc: 0.5777 - val_loss: 1.1380 - val_acc: 0.6172\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.2076 - acc: 0.6040 - val_loss: 1.1194 - val_acc: 0.6404\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.1589 - acc: 0.6173 - val_loss: 1.1045 - val_acc: 0.6290\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.1296 - acc: 0.6274 - val_loss: 1.1081 - val_acc: 0.6356\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.0666 - acc: 0.6461 - val_loss: 1.1266 - val_acc: 0.6415\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 41s - loss: 1.0452 - acc: 0.6616 - val_loss: 1.0843 - val_acc: 0.6430\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.9968 - acc: 0.6733 - val_loss: 1.1082 - val_acc: 0.6452\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.9635 - acc: 0.6794 - val_loss: 1.1137 - val_acc: 0.6430\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.9295 - acc: 0.6929 - val_loss: 1.0915 - val_acc: 0.6577\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.8968 - acc: 0.7106 - val_loss: 1.0650 - val_acc: 0.6691\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.8736 - acc: 0.7169 - val_loss: 1.0752 - val_acc: 0.6584\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.8362 - acc: 0.7277 - val_loss: 1.1254 - val_acc: 0.6684\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.8150 - acc: 0.7372 - val_loss: 1.0995 - val_acc: 0.6647\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.7863 - acc: 0.7441 - val_loss: 1.1416 - val_acc: 0.6691\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.7695 - acc: 0.7554 - val_loss: 1.0863 - val_acc: 0.6636\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.7463 - acc: 0.7636 - val_loss: 1.1443 - val_acc: 0.6728\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.7187 - acc: 0.7756 - val_loss: 1.1617 - val_acc: 0.6669\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.6699 - acc: 0.7835 - val_loss: 1.1549 - val_acc: 0.6684\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.6501 - acc: 0.7916 - val_loss: 1.1510 - val_acc: 0.6743\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.6409 - acc: 0.7956 - val_loss: 1.1713 - val_acc: 0.6724\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.6266 - acc: 0.7974 - val_loss: 1.1723 - val_acc: 0.6728\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.6113 - acc: 0.8049 - val_loss: 1.1945 - val_acc: 0.6724\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.5781 - acc: 0.8185 - val_loss: 1.2206 - val_acc: 0.6526\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.5753 - acc: 0.8201 - val_loss: 1.1960 - val_acc: 0.6669\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.5493 - acc: 0.8264 - val_loss: 1.1893 - val_acc: 0.6706\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.5172 - acc: 0.8359 - val_loss: 1.2241 - val_acc: 0.6643\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.5145 - acc: 0.8421 - val_loss: 1.2431 - val_acc: 0.6732\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.4907 - acc: 0.8491 - val_loss: 1.2786 - val_acc: 0.6702\n",
      "Epoch 35/50\n",
      "1920/8000 [======>.......................] - ETA: 29s - loss: 0.4801 - acc: 0.8536"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
