{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SemEval2010_task8_training/TRAIN_FILE.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    sentences = s.splitlines()[0::4]\n",
    "    x_train =[sentence.split('\\t')[1].strip('\"') for sentence in sentences]\n",
    "    y_train = s.splitlines()[1::4]\n",
    "\n",
    "with open('SemEval2010_task8_testing_keys/TEST_FILE_CLEAN.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    x_test = []\n",
    "    for line in s.splitlines():\n",
    "        d = line.split('\\t')[1].strip('\"')\n",
    "        x_test.append(d)\n",
    "\n",
    "with open('../SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT','r') as f:\n",
    "    s = f.read()\n",
    "    y_test = [k.split('\\t')[1] for k in s.splitlines()]\n",
    "\n",
    "label_index = {'Cause-Effect': 0,\n",
    " 'Component-Whole': 1,\n",
    " 'Content-Container': 2,\n",
    " 'Entity-Destination': 3,\n",
    " 'Entity-Origin': 4,\n",
    " 'Instrument-Agency': 5,\n",
    " 'Member-Collection': 6,\n",
    " 'Message-Topic': 7,\n",
    " 'Other': 8,\n",
    " 'Product-Producer': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the labels as well as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "tokenizer = Tokenizer()\n",
    "def clean_text(sentence_list): \n",
    "    soups = [BeautifulSoup(sen,\"html5lib\") for sen in sentence_list]\n",
    "    clean_text = [soup.text for soup in soups]\n",
    "    e1_words = [soup.find('e1').text.lower() for soup in soups]\n",
    "    e2_words = [soup.find('e2').text.lower() for soup in soups]\n",
    "    clean_text = [' '.join(text_to_word_sequence(clean)) for clean in clean_text]\n",
    "    return [clean_text,e1_words,e2_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 85\n",
    "def pad(sequence):\n",
    "    return keras.preprocessing.sequence.pad_sequences(sequence,maxlen=MAX_LEN)\n",
    "\n",
    "def preprocess(params, tokenizer = tokenizer):  \n",
    "    [clean_train, e1, e2] = params\n",
    "    token_sentences = tokenizer.texts_to_sequences(clean_train)\n",
    "    token_e1 = tokenizer.texts_to_sequences(e1)\n",
    "    token_e2 = tokenizer.texts_to_sequences(e2)\n",
    "    e1_one_hots = []\n",
    "    for sentence,e1 in zip(token_sentences,token_e1):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e1:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e1_one_hots.append(s1)\n",
    "\n",
    "    e2_one_hots = []\n",
    "    for sentence,e2 in zip(token_sentences,token_e2):\n",
    "        s1 = []\n",
    "        for word in sentence:\n",
    "            if word in e2:\n",
    "                s1.append(1)\n",
    "            else:\n",
    "                s1.append(0)\n",
    "        e2_one_hots.append(s1)\n",
    "    padded_sentences = pad(token_sentences)\n",
    "    padded_e1 = pad(token_e1)\n",
    "    padded_e2 = pad(token_e2)\n",
    "    return [padded_sentences, padded_e1, padded_e2]\n",
    "\n",
    "def preprocess_labels(target, label_index=label_index):\n",
    "    reduced_labels = [t.replace('(e2,e1)','') for t in target]\n",
    "    reduced_labels = [t.replace('(e1,e2)','') for t in reduced_labels]\n",
    "    labels = [label_index[i] for i in reduced_labels]\n",
    "    labels = keras.utils.to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = clean_text(x_train)\n",
    "tokenizer.fit_on_texts(x_train[0])\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(clean_text(x_test))\n",
    "\n",
    "y_train = preprocess_labels(y_train)\n",
    "y_test = preprocess_labels(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a word embeddings Matrix (GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec matrix\n"
     ]
    }
   ],
   "source": [
    "GLOVE_URL = 'https://drive.google.com/uc?export=download&id=0B30g1WfHiiY-QloxTldQTkxVelU'\n",
    "if not 'glove.6B.100d.txt' in os.listdir('.'):\n",
    "    print(\"word2vec matrix not found. Downloading...\")\n",
    "    wget.download(GLOVE_URL)\n",
    "else:\n",
    "    print(\"Loading word2vec matrix...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt'),'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Sentences: (8000, 85)\n",
      "Labels: (8000, 10)\n",
      "Test Data:\n",
      "Sentences: (2717, 85)\n",
      "Labels: (8000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "print(\"Sentences: {}\".format(x_train[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(\"Sentences: {}\".format(x_test[0].shape))\n",
    "print(\"Labels: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Model with e1 and e2 as aux inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence input is being passed along with e1 and e2 as auxillary vectors. \n",
    "\n",
    "Eg: \n",
    "\n",
    "```arrayed <e1>configuration</e1> of antenna <e2>elements</e2>\n",
    "``` \n",
    "\n",
    "will be sent as \n",
    "\n",
    "|  |   |   |   |   |   \n",
    "|---|---|---|---|---|---\n",
    "||arrayed|configuration|of|antenna|elements|\n",
    "|*token*|10|44|5|24|104|\n",
    "|*e1*|0|1|0|0|0|\n",
    "|*e2*|0|0|0|0|1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")\n",
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)\n",
    "embed = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed(sentence_input)\n",
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")\n",
    "x = keras.layers.Dropout(0.2)(merged)\n",
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.LSTM(32)(x)\n",
    "x = keras.Dropout\n",
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)\n",
    "model = keras.models.Model([sentence_input,e1,e2],prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_cells (LSTM)                (None, 100)           81200       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 100)           0           LSTM_cells[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            1010        dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,042,810\n",
      "Trainable params: 82,210\n",
      "Non-trainable params: 1,960,600\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 37s - loss: 2.0581 - acc: 0.2645 - val_loss: 1.7221 - val_acc: 0.4071\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 35s - loss: 1.7140 - acc: 0.4039 - val_loss: 1.4862 - val_acc: 0.4829\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 33s - loss: 1.5343 - acc: 0.4762 - val_loss: 1.3802 - val_acc: 0.5171\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 38s - loss: 1.4197 - acc: 0.5163 - val_loss: 1.3337 - val_acc: 0.5480\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 35s - loss: 1.3356 - acc: 0.5400 - val_loss: 1.2716 - val_acc: 0.5605\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 35s - loss: 1.2588 - acc: 0.5627 - val_loss: 1.2410 - val_acc: 0.5664\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 35s - loss: 1.2179 - acc: 0.5772 - val_loss: 1.2087 - val_acc: 0.5907\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 36s - loss: 1.1564 - acc: 0.6019 - val_loss: 1.2022 - val_acc: 0.5786\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 34s - loss: 1.1235 - acc: 0.6144 - val_loss: 1.1996 - val_acc: 0.5867\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 35s - loss: 1.0841 - acc: 0.6199 - val_loss: 1.1409 - val_acc: 0.6047\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 38s - loss: 1.0335 - acc: 0.6384 - val_loss: 1.1403 - val_acc: 0.6088\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 36s - loss: 1.0025 - acc: 0.6492 - val_loss: 1.1293 - val_acc: 0.6158\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 35s - loss: 0.9765 - acc: 0.6556 - val_loss: 1.1253 - val_acc: 0.6146\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 34s - loss: 0.9324 - acc: 0.6746 - val_loss: 1.1379 - val_acc: 0.6110\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.9032 - acc: 0.6853 - val_loss: 1.1158 - val_acc: 0.6250\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 48s - loss: 0.8728 - acc: 0.6921 - val_loss: 1.1113 - val_acc: 0.6308\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 45s - loss: 0.8499 - acc: 0.7061 - val_loss: 1.1481 - val_acc: 0.6198\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 37s - loss: 0.8257 - acc: 0.7125 - val_loss: 1.1365 - val_acc: 0.6224\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 36s - loss: 0.7924 - acc: 0.7233 - val_loss: 1.1238 - val_acc: 0.6371\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 37s - loss: 0.7686 - acc: 0.7336 - val_loss: 1.1345 - val_acc: 0.6294\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 59s - loss: 0.7501 - acc: 0.7392 - val_loss: 1.1744 - val_acc: 0.6268\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.7208 - acc: 0.7473 - val_loss: 1.2168 - val_acc: 0.6209\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 41s - loss: 0.7058 - acc: 0.7498 - val_loss: 1.1848 - val_acc: 0.6227\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.6784 - acc: 0.7614 - val_loss: 1.1502 - val_acc: 0.6312\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.6577 - acc: 0.7725 - val_loss: 1.2111 - val_acc: 0.6308\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 45s - loss: 0.6453 - acc: 0.7715 - val_loss: 1.2076 - val_acc: 0.6353\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 44s - loss: 0.6262 - acc: 0.7826 - val_loss: 1.1985 - val_acc: 0.6356\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 44s - loss: 0.6045 - acc: 0.7901 - val_loss: 1.2208 - val_acc: 0.6238\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 44s - loss: 0.5959 - acc: 0.7896 - val_loss: 1.2295 - val_acc: 0.6316\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 49s - loss: 0.5638 - acc: 0.8014 - val_loss: 1.2394 - val_acc: 0.6375\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.5541 - acc: 0.8092 - val_loss: 1.2767 - val_acc: 0.6301\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 45s - loss: 0.5384 - acc: 0.8075 - val_loss: 1.2944 - val_acc: 0.6257\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 45s - loss: 0.5267 - acc: 0.8125 - val_loss: 1.2963 - val_acc: 0.6353\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 47s - loss: 0.5080 - acc: 0.8191 - val_loss: 1.2935 - val_acc: 0.6297\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 69s - loss: 0.5094 - acc: 0.8216 - val_loss: 1.3195 - val_acc: 0.6246\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 48s - loss: 0.4725 - acc: 0.8357 - val_loss: 1.3938 - val_acc: 0.6191\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.4671 - acc: 0.8394 - val_loss: 1.3237 - val_acc: 0.6290\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 44s - loss: 0.4576 - acc: 0.8369 - val_loss: 1.3549 - val_acc: 0.6297\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 38s - loss: 0.4467 - acc: 0.8452 - val_loss: 1.3676 - val_acc: 0.6275\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 38s - loss: 0.4392 - acc: 0.8445 - val_loss: 1.4097 - val_acc: 0.6319\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.4316 - acc: 0.8480 - val_loss: 1.3879 - val_acc: 0.6283\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 39s - loss: 0.4018 - acc: 0.8638 - val_loss: 1.3771 - val_acc: 0.6367\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.4043 - acc: 0.8555 - val_loss: 1.5188 - val_acc: 0.6143\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.3977 - acc: 0.8608 - val_loss: 1.4697 - val_acc: 0.6172\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 48s - loss: 0.3830 - acc: 0.8674 - val_loss: 1.4348 - val_acc: 0.6319\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.3812 - acc: 0.8659 - val_loss: 1.4144 - val_acc: 0.6338\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 42s - loss: 0.3725 - acc: 0.8659 - val_loss: 1.4626 - val_acc: 0.6294\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 40s - loss: 0.3585 - acc: 0.8745 - val_loss: 1.4812 - val_acc: 0.6334\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 43s - loss: 0.3453 - acc: 0.8770 - val_loss: 1.5311 - val_acc: 0.6268\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 45s - loss: 0.3331 - acc: 0.8858 - val_loss: 1.5058 - val_acc: 0.6327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1680dd91f60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "2. https://keras.io/getting-started/functional-api-guide/\n",
    "3. https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "sentence_input (InputLayer)      (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e1_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "e2_input (InputLayer)            (None, 85)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "word2vec (Embedding)             (None, 85, 100)       1960600     sentence_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "e1_reshape (Reshape)             (None, 85, 1)         0           e1_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "e2_reshape (Reshape)             (None, 85, 1)         0           e2_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge (Concatenate)              (None, 85, 102)       0           word2vec[0][0]                   \n",
      "                                                                   e1_reshape[0][0]                 \n",
      "                                                                   e2_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 85, 102)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_1 (LSTM)                    (None, 85, 128)       118272      dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 85, 128)       0           LSTM_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                    (None, 32)            20608       dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 32)            0           lstm_9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 10)            330         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,099,810\n",
      "Trainable params: 2,099,810\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sentence_input = keras.layers.Input(shape=(MAX_LEN,), name=\"sentence_input\")\n",
    "e1 = keras.layers.Input(shape=(MAX_LEN,), name=\"e1_input\")\n",
    "e1r = keras.layers.Reshape((MAX_LEN,1), name=\"e1_reshape\")(e1)\n",
    "e2 = keras.layers.Input(shape=(MAX_LEN,), name=\"e2_input\")\n",
    "e2r = keras.layers.Reshape((MAX_LEN,1), name=\"e2_reshape\")(e2)\n",
    "embed = keras.layers.Embedding(len(word_index)+1,100,weights=[embedding_matrix],input_length=MAX_LEN, name=\"word2vec\")\n",
    "vector_sentence = embed(sentence_input)\n",
    "merged = keras.layers.concatenate([vector_sentence,e1r,e2r], name=\"merge\")\n",
    "x = keras.layers.Dropout(0.2)(merged)\n",
    "x = keras.layers.LSTM(128,return_sequences=True,name=\"LSTM_1\")(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.LSTM(32)(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "prediction = keras.layers.Dense(10, activation='softmax', name=\"prediction\")(x)\n",
    "model = keras.models.Model([sentence_input,e1,e2],prediction)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2717 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 79s - loss: 1.8166 - acc: 0.3669 - val_loss: 1.4517 - val_acc: 0.5101\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.4447 - acc: 0.5075 - val_loss: 1.3458 - val_acc: 0.5414\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 85s - loss: 1.2724 - acc: 0.5659 - val_loss: 1.2160 - val_acc: 0.5830\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.1499 - acc: 0.6138 - val_loss: 1.1568 - val_acc: 0.6117\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 81s - loss: 1.0331 - acc: 0.6529 - val_loss: 1.0913 - val_acc: 0.6205\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 76s - loss: 0.9509 - acc: 0.6824 - val_loss: 1.1134 - val_acc: 0.6213\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 79s - loss: 0.8478 - acc: 0.7130 - val_loss: 1.1277 - val_acc: 0.6294\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 86s - loss: 0.7750 - acc: 0.7460 - val_loss: 1.0857 - val_acc: 0.6404\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 87s - loss: 0.6983 - acc: 0.7728 - val_loss: 1.1256 - val_acc: 0.6397\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 83s - loss: 0.6289 - acc: 0.7881 - val_loss: 1.1359 - val_acc: 0.6463\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 83s - loss: 0.5708 - acc: 0.8131 - val_loss: 1.1226 - val_acc: 0.6621\n",
      "Epoch 12/50\n",
      "2240/8000 [=======>......................] - ETA: 54s - loss: 0.4673 - acc: 0.8451"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
